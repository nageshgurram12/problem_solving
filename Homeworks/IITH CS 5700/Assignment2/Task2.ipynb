{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "from html.parser import HTMLParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some variables to use in code\n",
    "data_file1 = \"./data/D1.csv\"; # filepath for data1\n",
    "data_file2 = \"./data/D2.csv\"; # filepath for data2\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the csv file and retrieve all posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "  '''\n",
    "  Read the csv file and return all posts\n",
    "  '''\n",
    "  all_posts = [];\n",
    "  with open(file, \"rt\", encoding=\"utf8\") as datafile:\n",
    "    next(datafile)\n",
    "    csvreader = csv.reader(datafile)\n",
    "    for row in csvreader:\n",
    "      all_posts.append(row)\n",
    "  return all_posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the text and code tags seperately with html parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLDataParser(HTMLParser):\n",
    "  '''\n",
    "  Parse the html into two types of text\n",
    "  1) code\n",
    "  2) normal text\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    # initialize the base class\n",
    "    HTMLParser.__init__(self);\n",
    "    \n",
    "    self.code_content = \"\";\n",
    "    self.text_content = \"\";\n",
    "    self.is_code_tag = False;\n",
    "    \n",
    "  def handle_starttag(self, tag, attrs):\n",
    "    if tag == \"code\":\n",
    "      self.is_code_tag = True;\n",
    "    else:\n",
    "      for attr in attrs:\n",
    "        if attr[1] is not None:\n",
    "          self.text_content = self.text_content + attr[1]\n",
    "        \n",
    "  def handle_endtag(self, tag):\n",
    "    if(tag == \"code\"):\n",
    "      self.is_code_tag = False;\n",
    "  \n",
    "  def handle_data(self, data):\n",
    "    if self.is_code_tag:\n",
    "      self.code_content = self.code_content + data;\n",
    "    else:\n",
    "      self.text_content = self.text_content + data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Step1 : Data Clean </h4> \n",
    "<p>\n",
    "   We clean the data by doing\n",
    "    <ol>\n",
    "        <li> split text and code into seperate parts </li>\n",
    "        <li> trim and remove punctuation in text </li>\n",
    "        <li> lowercase all the text </li>\n",
    "        </ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(title, body):\n",
    "  '''\n",
    "  Parse the text as html and split into code and text parts\n",
    "  Then remove punctuation marks and convert to lower case\n",
    "  '''\n",
    "  html_parser = HTMLDataParser()\n",
    "  if body is not None:\n",
    "    html_parser.unescape(body)\n",
    "    html_parser.feed(body)\n",
    "  \n",
    "  text = title + html_parser.text_content\n",
    "  code = html_parser.code_content\n",
    "  \n",
    "  text = text.lower()\n",
    "  #remove the punctuation using the character deletion step of translate\n",
    "  no_punctuation = text.translate(str.maketrans('','', string.punctuation))\n",
    "  text_tokens = word_tokenize(no_punctuation)\n",
    "  \n",
    "  no_punctuation_code = code.translate(str.maketrans('','',string.punctuation))\n",
    "  code_tokens = word_tokenize(no_punctuation_code)\n",
    "  \n",
    "  return (text_tokens, code_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Step 2: Remove stop words </h4>\n",
    "    <p> We can remove stop words only for text but not for code as tokens in code are very syntactic. </p>\n",
    "    \n",
    "<h4> Step 3. Stemming </h4>\n",
    "<p> Apply stemming to tokens in text but not for source code since it has a predefined structure. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_and_do_stemming(text_tokens, code_tokens):\n",
    "  stemmed = []\n",
    "  for token in text_tokens:\n",
    "    if not token in stop_words:\n",
    "      stemmed.append(stemmer.stem(token))\n",
    "        \n",
    "  return (stemmed, code_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the data:\n",
    "\n",
    "<p> Pre process the data by applying the above <b>three steps</b> one by one and then filter out unique words in entire corpus as they don't add any value for document similarity. </p>\n",
    "<p> Based on observations in Task-1, we can set filter size to 15000 for dataset-1 and 60000 for dataset-2 </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_doc(title=\"\", body=\"\"):\n",
    "  '''\n",
    "  preprocess each document by using above steps\n",
    "  '''\n",
    "  #1. clean the data\n",
    "  #2. remove stop words\n",
    "  #3. do stemming\n",
    "  (text_tokens, code_tokens) = data_clean(title, body)\n",
    "  (text_tokens, code_tokens) = remove_stopwords_and_do_stemming(text_tokens, code_tokens);\n",
    "  # concat both text and code into one list and send\n",
    "  text_tokens = text_tokens + code_tokens\n",
    "  return text_tokens\n",
    "\n",
    "def filter_unique_words(corpus, k=10000):\n",
    "  # first get a list of all words\n",
    "  all_tokens = [token for doc in corpus for token in doc]\n",
    "  # use nltk fdist to get a frequency distribution of all words\n",
    "  fdist = FreqDist(all_tokens)\n",
    "  \n",
    "  #Create a set of top k tokens\n",
    "  top_k_tokens,_ = zip(*fdist.most_common(k))\n",
    "  top_k_tokens = set(top_k_tokens)\n",
    "  \n",
    "  # Filter the tokens that are not present in top k\n",
    "  for doc in corpus:\n",
    "    doc = filter(lambda token: token in top_k_tokens, doc)\n",
    "      \n",
    "  return corpus\n",
    "  \n",
    "def preprocess_corpus(corpus, k):\n",
    "  '''\n",
    "  Take the corpus as input and filter out tokens that are not in top k frequent\n",
    "  '''\n",
    "  tokenized_corpus= []\n",
    "  for doc in corpus:\n",
    "    tokenized_corpus.append(preprocess_doc(doc[0], doc[1]))\n",
    "  \n",
    "  filtered_tokenized_corpus = filter_unique_words(tokenized_corpus, k)\n",
    "  return filtered_tokenized_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build TF-IDF matrix for both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tf_idf(corpus, k):\n",
    "  tokens = preprocess_corpus(corpus, k)\n",
    "  # No need of tokenization and lower case as we already pre-processed\n",
    "  tf_idf = TfidfVectorizer(tokenizer=lambda x:x, lowercase=False, ngram_range=(1,3)) \n",
    "  \n",
    "  return (tf_idf, tf_idf.fit_transform(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and prepare corpus\n",
    "corpus1 = read_data(data_file1)\n",
    "corpus2 = read_data(data_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrgurram\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# Prepare tf-idf matrix for both corpuses ( this take a while)\n",
    "(tf_idf1, data1_vec) = build_tf_idf(corpus1, k=15000)\n",
    "(tf_idf2, data2_vec) = build_tf_idf(corpus2, k=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(query_tf_idf, corpus_tf_idf, k=5):\n",
    "  '''\n",
    "  calculate cosine similarity and get the top k similar posts indices\n",
    "  '''\n",
    "  cosine_similarities = linear_kernel(query_tf_idf, corpus_tf_idf).flatten()\n",
    "  related_docs_indices = cosine_similarities.argsort()[:-k-1:-1]\n",
    "  return related_docs_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Query match: </h4>\n",
    "<p>\n",
    "Given a query post with title and body, calculate TF-IDF score in the vector space of already prepared corpus.\n",
    "And then take cosine similarity between query post vector and corpus to result top k matches.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To match with existing questions already in corpus, enter dataset no.,  title and body . \n",
      "\n",
      "\n",
      "Dataset 1 (or) 2 : \n",
      "2\n",
      "\n",
      "Enter the title to match :\n",
      "What can be saved before factory reset?\n",
      "\n",
      "Enter the body to match :\n",
      "\"<p>My phone has a GPS problem and service says the need to wipe it completely.<br> They are probably just lazy and would rather do it the easy way.<br> C'est la vie.</p>  <p>The phone is SGS unrooted 2.3.4 - Touch Wiz. I don't want to root it.</p>  <p>Now, what I am going to do is this:<br> - Save contacts with Kies<br> - Save files</p>  <p>Is there anything else I can do or that I should know?</p>  <p>For example, I believe that apps associated with the Google Account will be reinstalled after I re-enter my account into the newly formatted phone, correct? However, app data like savegames won't be ported, I have to search for them in the phone memory?</p>  <p>Can I backup SMS?</p>  <p>Can I back-up settings?</p>  <p>Will imported contacts keep all fields like I have them now, i.e. work phone, home phone, work email, home email etc?</p>  <p>I know it's a ton of questions, sorry about that. Thank you.</p> \"\n",
      "\n",
      "To show only matched posts titles, press 1 \n",
      " or to see both title & body press 2: \n",
      "1\n",
      "Enter the number of top posts that matched to show :\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrgurram\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: The unescape method is deprecated and will be removed in 3.5, use html.unescape() instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar posts matching the query (best match at top) : \n",
      "\n",
      "\n",
      "Title : What can be saved before factory reset?\n",
      "---------------------------\n",
      "Title : How to add new contacts to an outlook.com account in an Android device?\n",
      "---------------------------\n",
      "Title : Saved my contacts in google account but they don't appear on my new phone\n",
      "---------------------------\n",
      "Title : Android 7 - How to save new contacts to the phone\n",
      "---------------------------\n",
      "Title : Is it possible to backup settings and apps for SGS with Samsung Kies?\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"To match with existing questions already in corpus, enter dataset no.,  title and body . \\n\\n\")\n",
    "inp_dataset = input(\"Dataset 1 (or) 2 : \\n\")\n",
    "inp_title = input(\"\\nEnter the title to match :\\n\")\n",
    "inp_body = input(\"\\nEnter the body to match :\\n\")\n",
    "\n",
    "show_tb = input(\"\\nTo show only matched posts titles, press 1 \\n or to see both title & body press 2: \\n\")\n",
    "\n",
    "show_topk =  input(\"Enter the number of top posts that matched to show :\\n\")\n",
    "\n",
    "if inp_dataset == \"1\":\n",
    "  (tf_idf, data_vec, corpus) = (tf_idf1, data1_vec, corpus1)\n",
    "elif inp_dataset == \"2\":\n",
    "  (tf_idf, data_vec, corpus) = (tf_idf2, data2_vec, corpus2)\n",
    "else:\n",
    "  raise Exception(\"Input data set option is 1 or 2\")\n",
    "\n",
    "query_tokens = preprocess_doc(inp_title, inp_body) # Preprocess the input query title and body\n",
    "query_vec = tf_idf.transform([query_tokens])\n",
    "indices = cosine_similarity(query_vec, data_vec, int(show_topk)) # Take cosine similarity between query and posts\n",
    "\n",
    "print(\"Similar posts matching the query (best match at top) : \\n\\n\")\n",
    "for i in indices:\n",
    "  print(\"Title : \" + corpus[i][0])\n",
    "  if show_tb == 2:\n",
    "    print(\"Body : \" + corpus[i][1])\n",
    "  print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic modeling with LDA:\n",
    "\n",
    "<h5>Observations : </h5>\n",
    "<p style=\"text-decoration: underline\">\n",
    "As the corpus is taken from single topic (dataset1 is from AI/machine learning and dataset2 is about Android), all the documents are mapped to single topic. For this reason, query filtering in this corpus using topic modeling is not efficient.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nrgurram\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query topic: 0\n",
      "10 random documents topics: [0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=10, n_jobs=-1, random_state=0)\n",
    "doc_topic_prob = lda.fit_transform(data_vec)\n",
    "\n",
    "query_topic = lda.transform(query_vec).argmax()\n",
    "rand_doc_topics = []\n",
    "for i in np.random.randint(0, data_vec.shape[0], 4):\n",
    "  rand_doc_topics.append(doc_topic_prob[i].argmax())\n",
    "  \n",
    "print(\"query topic: \" + str(query_topic))\n",
    "print(\"10 random documents topics: \" + str(rand_doc_topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
